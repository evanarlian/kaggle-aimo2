import asyncio
import json
import os
import time
from collections import Counter
from itertools import islice
from pathlib import Path
from typing import Optional

import polars as pl
from openai import AsyncOpenAI
from pydantic import BaseModel
from transformers import AutoTokenizer, PreTrainedTokenizer

import kaggle_evaluation.aimo_2_inference_server
from aimo2.parser.latex import MyLatexParser, extract_boxed_text
from aimo2.timer import Timer
from aimo2.utils import is_kaggle, wib_now


class Config(BaseModel):
    n_parallel: int  # launch n parallel calls to vllm per question
    n_first: int  # consider only the top n_first, and discard the slowest ones
    model: str  # single model to call
    grammar_dir: Path  # path to modified latex grammar
    num_questions: int  # number of questions for timing purposes
    hours: float  # total competition hours
    exp_path: Path  # experiment folder
    source_csv: str  # the test csv path for kaggle inference server


class ConversationResult(BaseModel):
    """Core inference result"""

    llm_answer: str  # raw unmodified llm output
    n_tokens: int  # total tokens of the raw llm output
    boxed_answer: Optional[str]  # raw answer generated by llm
    parsed_answer: Optional[int]  # parsed answer using sympy after modulus
    temperature: float
    top_p: float
    min_p: float


class ConversationReport(ConversationResult):
    """Wraps core and adding admin stuffs"""

    q_id: str
    question: str
    gt_answer: Optional[int]
    elapsed: float
    tok_per_sec: float


async def conversation(
    q_text: str,
    q_id: str,
    client: AsyncOpenAI,
    tokenizer: PreTrainedTokenizer,
    parser: MyLatexParser,
    cfg: Config,
) -> ConversationResult:
    """Core conversation logic"""
    # 0. randomizer
    # TODO make random later? we need speed tho
    temperature = 1.0
    top_p = 0.9
    min_p = 0.1
    # 1. get answer initial answer
    history: list = [{"role": "user", "content": q_text}]
    completion1 = await client.chat.completions.create(
        model=cfg.model,
        messages=history,
        temperature=temperature,
        top_p=top_p,
        stop=["</think>"],  # don't waste time on repeating what the model knows
        extra_body={"min_p": min_p},
    )
    assert completion1.choices[0].message.content is not None
    assert completion1.usage is not None
    reply1 = completion1.choices[0].message.content
    n_toks = completion1.usage.completion_tokens
    history.append({"role": "assistant", "content": reply1})
    # 2. force the model to output in the right format if not exist
    box_content1 = extract_boxed_text(reply1)
    print(f"[{q_id}] box_content1: {box_content1}")
    if box_content1 is None:
        # try to fix it once
        history[-1]["content"] = (
            history[-1]["content"].replace("</think>", "<think2>")
            + "\n\n**Final Answer:**\n\\[\n\\boxed{"
        )
        raw = tokenizer.apply_chat_template(
            history,
            tokenize=False,
            add_generation_prompt=False,
            continue_final_message=True,
        ).replace("<think2>", "</think>")  # type: ignore
        completion2 = await client.completions.create(
            model=cfg.model,
            prompt=raw,
            temperature=temperature,
            top_p=top_p,
            extra_body={"min_p": min_p},
        )
        reply2 = history[-1]["content"] + completion2.choices[0].text
        history[-1]["content"] = reply2
        box_content2 = extract_boxed_text(reply2)
        print(f"[{q_id}] box_content2: {box_content2}")
        if box_content2 is None:
            # still no box after this, just bail
            return ConversationResult(
                llm_answer=reply1,
                n_tokens=n_toks,
                boxed_answer=None,
                parsed_answer=None,
                temperature=temperature,
                top_p=top_p,
                min_p=min_p,
            )
        box_content1 = box_content2
    # 3. extract
    predicted_num = parser.latex_to_int_modded(box_content1)
    print(f"[{q_id}] predicted_num: {predicted_num}")
    if predicted_num is None:
        # unparsable number here
        return ConversationResult(
            llm_answer=reply1,
            n_tokens=n_toks,
            boxed_answer=box_content1,
            parsed_answer=None,
            temperature=temperature,
            top_p=top_p,
            min_p=min_p,
        )
    print(f"[{q_id}] convo done!")
    return ConversationResult(
        llm_answer=reply1,
        n_tokens=n_toks,
        boxed_answer=box_content1,
        parsed_answer=predicted_num,
        temperature=temperature,
        top_p=top_p,
        min_p=min_p,
    )


async def worker(
    q_text: str,
    q_id: str,
    gt_answer: Optional[int],
    voting: Counter,
    client: AsyncOpenAI,
    tokenizer: PreTrainedTokenizer,
    parser: MyLatexParser,
    cfg: Config,
) -> ConversationReport:
    """Wrapper for core, adding admin stuffs and doing the actual voting"""
    t0 = time.perf_counter()
    convo = await conversation(q_text, q_id, client, tokenizer, parser, cfg)
    elapsed = time.perf_counter() - t0
    if convo.parsed_answer is not None:
        voting[convo.parsed_answer] += 1
    return ConversationReport(
        **convo.model_dump(),
        q_id=q_id,
        question=q_text,
        gt_answer=gt_answer,
        elapsed=elapsed,
        tok_per_sec=convo.n_tokens / elapsed,
    )


async def solve_one(
    q_text: str,
    q_id: str,
    gt_answer: Optional[int],
    client: AsyncOpenAI,
    tokenizer: PreTrainedTokenizer,
    parser: MyLatexParser,
    timer: Timer,
    cfg: Config,
) -> int:
    """Manages workers (parallel calls to vllm)"""
    allowed_time = timer.start_question()
    print(f"[{q_id}] allowed time: {allowed_time}")
    voting = Counter()
    print(f"[{q_id}] creating {cfg.n_parallel} workers")
    worker_tasks = [
        worker(q_text, q_id, gt_answer, voting, client, tokenizer, parser, cfg)
        for _ in range(cfg.n_parallel)
    ]
    # only grab the fastest ones, but still under time limit
    convo_reports = []
    for done in islice(
        asyncio.as_completed(worker_tasks, timeout=allowed_time), cfg.n_first
    ):
        try:
            convo_reports.append(await done)
        except Exception as e:
            print(f"[{q_id}] timeout or raises: {e}")
    # save to json
    cfg.exp_path.parent.mkdir(parents=True, exist_ok=True)
    if cfg.exp_path.exists():
        with open(cfg.exp_path, "r") as f:
            existing = json.load(f)
    else:
        existing = []
    existing += [c.model_dump() for c in convo_reports]
    with open(cfg.exp_path, "w") as f:
        json.dump(existing, f, indent=4)
    print(f"[{q_id}] convos added to {cfg.exp_path}")
    try:
        answer, n_votes = voting.most_common(1)[0]
    except IndexError:
        print(f"[{q_id}] there are no votes at all, convos might be too long")
        answer = 0
    # complete!
    timer.finish_question()
    return answer


########################################################################################

if is_kaggle():
    cfg = Config(
        n_parallel=32,
        n_first=24,
        model="/kaggle/input/deepseek-r1/transformers/deepseek-r1-distill-qwen-7b-awq-casperhansen/1",
        grammar_dir=Path("parser"),
        num_questions=50,
        hours=5,
        exp_path=Path("experiments") / f"exp_maj_{wib_now()}.json",
        source_csv="/kaggle/input/ai-mathematical-olympiad-progress-prize-2/test.csv",
    )
else:
    cfg = Config(
        n_parallel=16,
        n_first=10,
        model="casperhansen/deepseek-r1-distill-qwen-1.5b-awq",
        grammar_dir=Path("aimo2/parser"),
        num_questions=3,
        hours=0.3,
        exp_path=Path("experiments") / f"exp_maj_{wib_now()}.json",
        source_csv="data/test.csv",
    )
timer = Timer(
    n_questions=cfg.num_questions,
    time_limit=cfg.hours * 60 * 60 * 0.95,  # give a bit of a headroom
)
tokenizer = AutoTokenizer.from_pretrained(cfg.model)
parser = MyLatexParser(cfg.grammar_dir, mod=1000, timeout=2.0)
client = AsyncOpenAI(base_url="http://localhost:8000/v1", api_key="-")


# Replace this function with your inference code.
# The function should return a single integer between 0 and 999, inclusive.
# Each prediction (except the very first) must be returned within 30 minutes of the question being provided.
def predict(
    id_: pl.DataFrame, question: pl.DataFrame, answer: Optional[pl.DataFrame] = None
) -> pl.DataFrame:
    q_id = str(id_.item(0))
    q_text = str(question.item(0))
    q_answer = None if answer is None else int(answer.item(0))
    prediction = asyncio.run(
        solve_one(q_text, q_id, q_answer, client, tokenizer, parser, timer, cfg)
    )
    return pl.DataFrame({"id": id_, "answer": prediction})


def main():
    inference_server = kaggle_evaluation.aimo_2_inference_server.AIMO2InferenceServer(
        predict  # type: ignore
    )
    if os.getenv("KAGGLE_IS_COMPETITION_RERUN"):
        inference_server.serve()
    else:
        inference_server.run_local_gateway((cfg.source_csv,))
        # sanity check
        df = pl.read_parquet("submission.parquet")
        print(df)


if __name__ == "__main__":
    main()
