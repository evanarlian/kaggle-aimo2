import asyncio
import json
import os
import time
from itertools import islice
from pathlib import Path
from typing import Optional

import httpx
import polars as pl
from openai import AsyncOpenAI
from pydantic import BaseModel
from transformers import AutoTokenizer, PreTrainedTokenizer

import kaggle_evaluation.aimo_2_inference_server
from aimo2.parser.latex import MyLatexParser, extract_boxed_text
from aimo2.timer import Timer
from aimo2.utils import is_kaggle, wib_now


class ModelInfo(BaseModel):
    n_parallel: int  # launch n parallel calls to vllm per question
    name: str  # model name to call
    base_url: str  # vllm base url


class Config(BaseModel):
    model_1: ModelInfo  # reasoning
    model_2: ModelInfo  # reasoning
    model_3: ModelInfo  # reasoning
    model_prm: ModelInfo  # reward
    n_first: int  # consider only the top n_first, and discard the slowest ones
    grammar_dir: Path  # path to modified latex grammar
    num_questions: int  # number of questions for timing purposes
    hours: float  # total competition hours
    exp_path: Path  # experiment folder
    source_csv: str  # the test csv path for kaggle inference server


class ConversationResult(BaseModel):
    """Core inference result"""

    llm_answer: str  # raw unmodified llm output
    n_tokens: int  # total tokens of the raw llm output
    boxed_answer: Optional[str]  # raw answer generated by llm
    parsed_answer: Optional[int]  # parsed answer using sympy after modulus
    rewards: Optional[list[float]]  # PRM scores for steps after </think>
    temperature: float
    top_p: float
    min_p: float


class ConversationReport(ConversationResult):
    """Wraps core and adding admin stuffs"""

    q_id: str
    question: str
    gt_answer: Optional[int]
    elapsed: float
    tok_per_sec: float


async def call_prm_vllm(conversation_str: str, cfg: Config) -> list[float]:
    data = {"model": cfg.model_prm.name, "input": conversation_str}
    async with httpx.AsyncClient(timeout=None) as client:
        r = await client.post(f"{cfg.model_prm.base_url}/pooling", json=data)
        result = r.json()
    process_rewards = [a[1] for a in result["data"][0]["data"]]
    return process_rewards


async def get_process_rewards(
    question: str, llm_answer: str, tokenizer, cfg: Config
) -> list[float]:
    all_groups = []
    group = []
    curr_group_len = 0
    for sentence in llm_answer.strip().split("\n\n"):
        group.append(sentence)
        curr_group_len += len(tokenizer.tokenize(sentence))
        if curr_group_len > 3000:
            all_groups.append(group)
            group = []
            curr_group_len = 0
    if group != []:
        all_groups.append(group)

    rewards = []
    for group in all_groups:
        messages = [
            {"role": "user", "content": question},
            {"role": "assistant", "content": "<extra_0>".join(group) + "<extra_0>"},
        ]
        conversation_str = tokenizer.apply_chat_template(
            messages, tokenize=False, add_generation_prompt=False
        )
        rewards += await call_prm_vllm(conversation_str, cfg)
    return rewards


async def conversation(
    q_text: str,
    q_id: str,
    client: AsyncOpenAI,
    reasoning_model: str,
    prm_tokenizer: PreTrainedTokenizer,
    parser: MyLatexParser,
    cfg: Config,
) -> ConversationResult:
    """Core conversation logic"""
    # 0. randomizer
    # TODO make random later? we need speed tho
    temperature = 1.0
    top_p = 0.9
    min_p = 0.1
    # 1. get answer initial answer
    history: list = [
        {
            "role": "user",
            "content": "Please reason step by step, and put your final answer within \\boxed{}. "
            + q_text,
        }
    ]
    completion1 = await client.chat.completions.create(
        model=reasoning_model,
        messages=history,
        temperature=temperature,
        top_p=top_p,
        extra_body={"min_p": min_p},
    )
    assert completion1.choices[0].message.content is not None
    assert completion1.usage is not None
    reply = completion1.choices[0].message.content
    n_toks = completion1.usage.completion_tokens
    # 2. quick check for prm, valid answer must have </think> token
    if "</think>" not in reply:
        return ConversationResult(
            llm_answer=reply,
            n_tokens=n_toks,
            boxed_answer=None,
            parsed_answer=None,
            rewards=None,
            temperature=temperature,
            top_p=top_p,
            min_p=min_p,
        )
    history.append({"role": "assistant", "content": reply})
    # 3. try parsing the boxed answer
    box_content = extract_boxed_text(reply)
    print(f"[{q_id}] box_content1: {box_content}")
    if box_content is None:
        return ConversationResult(
            llm_answer=reply,
            n_tokens=n_toks,
            boxed_answer=None,
            parsed_answer=None,
            rewards=None,
            temperature=temperature,
            top_p=top_p,
            min_p=min_p,
        )
    # 4. try parse boxed content using sympy
    predicted_num = parser.latex_to_int_modded(box_content)
    print(f"[{q_id}] predicted_num: {predicted_num}")
    if predicted_num is None:
        return ConversationResult(
            llm_answer=reply,
            n_tokens=n_toks,
            boxed_answer=box_content,
            parsed_answer=None,
            rewards=None,
            temperature=temperature,
            top_p=top_p,
            min_p=min_p,
        )
    # 4. call reward model
    summarized_part = reply.split("</think>", maxsplit=1)[-1].strip()
    rewards = await get_process_rewards(q_text, summarized_part, prm_tokenizer, cfg)
    print(f"[{q_id}] prm done!")
    return ConversationResult(
        llm_answer=reply,
        n_tokens=n_toks,
        boxed_answer=box_content,
        parsed_answer=predicted_num,
        rewards=rewards,
        temperature=temperature,
        top_p=top_p,
        min_p=min_p,
    )


async def worker(
    q_text: str,
    q_id: str,
    gt_answer: Optional[int],
    client: AsyncOpenAI,
    reasoning_model: str,
    prm_tokenizer: PreTrainedTokenizer,
    parser: MyLatexParser,
    cfg: Config,
) -> ConversationReport:
    """Wrapper for core, doing admin stuffs"""
    t0 = time.perf_counter()
    convo = await conversation(
        q_text, q_id, client, reasoning_model, prm_tokenizer, parser, cfg
    )
    elapsed = time.perf_counter() - t0
    return ConversationReport(
        **convo.model_dump(),
        q_id=q_id,
        question=q_text,
        gt_answer=gt_answer,
        elapsed=elapsed,
        tok_per_sec=convo.n_tokens / elapsed,
    )


async def solve_one(
    q_text: str,
    q_id: str,
    gt_answer: Optional[int],
    prm_tokenizer: PreTrainedTokenizer,
    parser: MyLatexParser,
    timer: Timer,
    cfg: Config,
) -> int:
    """Manages workers (parallel calls to vllm)"""
    allowed_time = timer.start_question()
    print(f"[{q_id}] allowed time: {allowed_time}")
    # create multiple openai clients
    client1 = AsyncOpenAI(base_url=cfg.model_1.base_url, api_key="-")
    client2 = AsyncOpenAI(base_url=cfg.model_2.base_url, api_key="-")
    client3 = AsyncOpenAI(base_url=cfg.model_3.base_url, api_key="-")
    worker_tasks = []
    for _ in range(cfg.model_1.n_parallel):
        worker_tasks.append(worker(q_text, q_id, gt_answer, client1, cfg.model_1.name, prm_tokenizer, parser, cfg))  # fmt: skip
    for _ in range(cfg.model_2.n_parallel):
        worker_tasks.append(worker(q_text, q_id, gt_answer, client2, cfg.model_2.name, prm_tokenizer, parser, cfg))  # fmt: skip
    for _ in range(cfg.model_3.n_parallel):
        worker_tasks.append(worker(q_text, q_id, gt_answer, client3, cfg.model_3.name, prm_tokenizer, parser, cfg))  # fmt: skip
    # only grab the fastest ones, but still under time limit
    convo_reports = []
    for done in islice(
        asyncio.as_completed(worker_tasks, timeout=allowed_time), cfg.n_first
    ):
        try:
            convo_reports.append(await done)
        except Exception as e:
            print(f"[{q_id}] timeout or raises: {e}")
    # save to json
    cfg.exp_path.parent.mkdir(parents=True, exist_ok=True)
    if cfg.exp_path.exists():
        with open(cfg.exp_path, "r") as f:
            existing = json.load(f)
    else:
        existing = []
    existing += [c.model_dump() for c in convo_reports]
    with open(cfg.exp_path, "w") as f:
        json.dump(existing, f, indent=4)
    print(f"[{q_id}] convos added to {cfg.exp_path}")
    # do something with the rewards
    answer = -7  # TODO
    # complete!
    timer.finish_question()
    return answer


########################################################################################

if is_kaggle():
    # TODO configure n parallel
    cfg = Config(
        model_1=ModelInfo(
            n_parallel=12,
            name="/kaggle/input/deepseek-r1/transformers/deepseek-r1-distill-qwen-7b-awq-casperhansen/1",
            base_url="http://localhost:8001/v1",
        ),
        model_2=ModelInfo(
            n_parallel=12,
            name="/kaggle/input/open-r1/transformers/openr1-qwen-7b-awq/1/openr1_awq",
            base_url="http://localhost:8002/v1",
        ),
        model_3=ModelInfo(
            n_parallel=32,
            name="/kaggle/input/deepscaler/transformers/deepscaler-1.5b-preview-awq/1/deepscaler_awq",
            base_url="http://localhost:8003/v1",
        ),
        model_prm=ModelInfo(
            n_parallel=-1,  # does not apply
            name="/kaggle/input/qwen2.5-math/transformers/qwen2.5-math-prm-7b/1/qwen_math_prm",
            base_url="http://localhost:8004",  # pooling requires no "v1"
        ),
        n_first=24,  # TODO
        grammar_dir=Path("parser"),
        num_questions=50,
        hours=5,
        exp_path=Path("experiments") / f"exp_bon_{wib_now()}.json",
        source_csv="/kaggle/input/ai-mathematical-olympiad-progress-prize-2/test.csv",
    )
else:
    cfg = Config(
        model_1=ModelInfo(
            n_parallel=4,
            name="casperhansen/deepseek-r1-distill-qwen-1.5b-awq",
            base_url="http://localhost:8001/v1",
        ),
        model_2=ModelInfo(
            n_parallel=4,
            name="casperhansen/deepseek-r1-distill-qwen-1.5b-awq",
            base_url="http://localhost:8001/v1",
        ),
        model_3=ModelInfo(
            n_parallel=4,
            name="casperhansen/deepseek-r1-distill-qwen-1.5b-awq",
            base_url="http://localhost:8001/v1",
        ),
        model_prm=ModelInfo(
            n_parallel=-1,  # does not apply
            name="Qwen/Qwen2.5-Math-PRM-7B",
            base_url="http://localhost:8004",
        ),
        n_first=8,
        grammar_dir=Path("aimo2/parser"),
        num_questions=3,
        hours=0.3,
        exp_path=Path("experiments") / f"exp_bon_{wib_now()}.json",
        source_csv="data/test.csv",
    )
timer = Timer(
    n_questions=cfg.num_questions,
    time_limit=cfg.hours * 60 * 60 * 0.95,  # give a bit of a headroom
)
prm_tokenizer = AutoTokenizer.from_pretrained(cfg.model_prm.name)
parser = MyLatexParser(cfg.grammar_dir, mod=1000, timeout=2.0)


# Replace this function with your inference code.
# The function should return a single integer between 0 and 999, inclusive.
# Each prediction (except the very first) must be returned within 30 minutes of the question being provided.
def predict(
    id_: pl.DataFrame, question: pl.DataFrame, answer: Optional[pl.DataFrame] = None
) -> pl.DataFrame:
    q_id = str(id_.item(0))
    q_text = str(question.item(0))
    q_answer = None if answer is None else int(answer.item(0))
    prediction = asyncio.run(
        solve_one(q_text, q_id, q_answer, prm_tokenizer, parser, timer, cfg)
    )
    return pl.DataFrame({"id": id_, "answer": prediction})


def main():
    inference_server = kaggle_evaluation.aimo_2_inference_server.AIMO2InferenceServer(
        predict
    )
    if os.getenv("KAGGLE_IS_COMPETITION_RERUN"):
        inference_server.serve()
    else:
        inference_server.run_local_gateway((cfg.source_csv,))
        # sanity check
        df = pl.read_parquet("submission.parquet")
        print(df)


if __name__ == "__main__":
    main()
