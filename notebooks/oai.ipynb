{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from typing import Any\n",
    "import IPython.display as ipd\n",
    "from aimo2.utils import mdlatex\n",
    "from copy import deepcopy\n",
    "from aimo2.parser import extract_boxed_text, latex_to_int\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Optional\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer\n",
    "import time\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"casperhansen/deepseek-r1-distill-qwen-1.5b-awq\"\n",
    "model = \"evanarlian/DeepScaleR-1.5B-Preview-AWQ\"\n",
    "client = OpenAI(api_key=\"-\", base_url=\"http://localhost:8000/v1\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history: Any = [\n",
    "    # {\n",
    "    #     \"role\": \"system\",\n",
    "    #     # \"content\": \"Be very concise, terse, to the point, confident, and put final answer in \\\\boxed{}.\",\n",
    "    #     \"content\": \"Be concise and put final answer in \\\\boxed{}.\",\n",
    "    #     # \"content\": \"The system rewards shorter answer. Be very concise, terse, to the point, confident, and put final answer in \\\\boxed{}.\",\n",
    "    #     # \"content\": \"Write down the related math theorems first, then think, then put final answer in \\\\boxed{}.\",\n",
    "    #     # \"content\": \"Give me the simplest solution, then put final answer in \\\\boxed{}.\",\n",
    "    # },\n",
    "    # {\n",
    "    #     \"role\": \"user\",\n",
    "    #     \"content\": \"Three airline companies operate flights from Dodola island. Each company has a different schedule of departures. The first company departs every 100 days, the second every 120 days and the third every 150 days. What is the greatest positive integer $d$ for which it is true that there will be $d$ consecutive days without a flight from Dodola island, regardless of the departure times of the various airlines?\",\n",
    "    # },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"What is the first prime number?\",\n",
    "    },\n",
    "]\n",
    "t0 = time.perf_counter()\n",
    "completion = client.chat.completions.create(\n",
    "    model=model,\n",
    "    messages=history,\n",
    "    temperature=0.6,\n",
    "    logprobs=True,\n",
    "    # top_p=0.9,\n",
    "    # max_tokens=300,\n",
    "    # stop=[\"</think>\"],\n",
    "    extra_body={\n",
    "        \"min_p\": 0.1,\n",
    "    },\n",
    ")\n",
    "elapsed = time.perf_counter() - t0\n",
    "reply = completion.choices[0].message.content\n",
    "logprobs = np.array([l.logprob for l in completion.choices[0].logprobs.content])\n",
    "history.append({\"role\": \"assistant\", \"content\": reply})\n",
    "toks = completion.usage.completion_tokens\n",
    "tok_per_sec = toks / elapsed\n",
    "print(toks, tok_per_sec, logprobs.mean(), logprobs.std())  # 475-485\n",
    "mdlatex(reply)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_boxed_text(reply)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.sort(logprobs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(logprobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "completion.choices[0].logprobs.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in case the model does not make the final answer\n",
    "# use hacks because </think> crashes the apply_chat_template\n",
    "fake_history = deepcopy(history)\n",
    "fake_history[-1][\"content\"] = (\n",
    "    fake_history[-1][\"content\"].replace(\"</think>\", \"<think2>\")\n",
    "    + \"\\n\\nI repeat, the final answer is: \\\\[\\n\\\\boxed{\"\n",
    ")\n",
    "prompt = tokenizer.apply_chat_template(\n",
    "    fake_history,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=False,\n",
    "    continue_final_message=True,\n",
    ").replace(\"<think2>\", \"</think>\")\n",
    "completion = client.completions.create(\n",
    "    model=model,\n",
    "    prompt=prompt,\n",
    "    temperature=0.6,\n",
    "    # max_tokens=10000,\n",
    ")\n",
    "reply = fake_history[-1][\"content\"] + completion.choices[0].text\n",
    "# history.append({\"assistant\": reply})\n",
    "mdlatex(reply)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.tokenize(\"\\\\[\\n\\\\boxed{\\\\dfrac{5}{6}}\\n\\\\]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_boxed_text(text: str) -> Optional[int]:\n",
    "    pattern = r\"\\boxed{(.+?)}\"\n",
    "    matches = re.findall(pattern, text)\n",
    "    if not matches:\n",
    "        return \"\"\n",
    "    for match in matches[::-1]:\n",
    "        if match != \"\":\n",
    "            return match\n",
    "    return \"\"\n",
    "extract_boxed_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Final answer: \\\\boxed{hello123/sddf}\"\n",
    "pattern = r\"\\boxed{(.+?)}\"\n",
    "matches = re.findall(pattern, text)\n",
    "if not matches:\n",
    "    return \"\"\n",
    "for match in matches[::-1]:\n",
    "    if match != \"\":\n",
    "        return match\n",
    "return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "-900 % 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answer parsing\n",
    "* extract from \\\\boxed{..}\n",
    "* check if that is computable "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_boxed_content(latex_str):\n",
    "    stack = []\n",
    "    result = []\n",
    "    i = 0\n",
    "    while i < len(latex_str):\n",
    "        if latex_str[i:].startswith(\"\\\\boxed{\"):\n",
    "            i += 7  # Skip \"\\boxed{\"\n",
    "            start = i\n",
    "            stack = [1]\n",
    "            while i < len(latex_str) and stack:\n",
    "                if latex_str[i] == \"{\":\n",
    "                    stack.append(1)\n",
    "                elif latex_str[i] == \"}\":\n",
    "                    stack.pop()\n",
    "                i += 1\n",
    "            if not stack:\n",
    "                result.append(latex_str[start : i - 1])\n",
    "        else:\n",
    "            i += 1\n",
    "    return result\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Example\n",
    "latex_output = r\"The answer is \\(\\boxed{\\frac{3}{2} + \\sqrt{16}}\\)\"\n",
    "boxed_content = extract_boxed_content2(latex_output)\n",
    "print(boxed_content)  # Outputs: \\frac{3}{2} + \\sqrt{16}\n",
    "print(\"=============\")\n",
    "print(extract_boxed_content2(\"ss\"))  # Outputs: \\frac{3}{2} + \\sqrt{16}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_boxed_content(r\"\\(\\boxed{\\frac{3}{2}}\\)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
